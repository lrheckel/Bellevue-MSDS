---
title: "Assigment 9.1 Machine Learning"
author: "Larry Heckel"
date: May 11, 2019
output: html_document


---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Assignment 9.1 Instructions

Regression algorithms are used to predict numeric quantity while classification algorithms predict categorical outcomes. A spam filter is an example use case for a classification algorithm. The input dataset is emails labeled as either spam (i.e. junk emails) or ham (i.e. good emails). The classification algorithm uses features extracted from the emails to learn which emails fall into which category.

In this problem, you will use the nearest neighbors algorithm to fit a model on two simplified datasets. The first dataset (found in binary-classifier-data.csv) contains three variables; label, x, and y. The label variable is either 0 or 1 and is the output we want to predict using the x and y variables. The second dataset (found in trinary-classifier-data.csv) is similar to the first dataset except for the label variable can be 0, 1, or 2.

Note that in real-world datasets, your labels are usually not numbers, but text-based descriptions of the categories (e.g. spam or ham). In practice, you will encode categorical variables into numeric values.

  a. Plot the data from each dataset using a scatter plot.

  b. The k nearest neighbors algorithm categorizes an input value by looking at the labels for the
  k nearest points and assigning a category based on the most common label. In this problem, you
  will determine which points are nearest by calculating the Euclidean distance between two points.
  As a refresher, the Euclidean distance between two points: p1=(x1, y1) and p2=(x2,y2) is d= 

      Fitting a model is when you use the input data to create a predictive model. There are
      various metrics you can use to determine how well your model fits the data. You will learn
      more about these metrics in later lessons. For this problem, you will focus on a single
      metric; accuracy. Accuracy is simply the percentage of how often the model predicts the
      correct result. If the model always predicts the correct result, it is 100% accurate. If the
      model always predicts the incorrect result, it is 0% accurate.

      Fit a k nearest neighbors model for each dataset for k=3, k=5, k=10, k=15, k=20, and k=25.
      Compute the accuracy of the resulting models for each value of k. Plot the results in a graph
      where the x-axis is the different values of k and the y-axis is the accuracy of the model.

  c. In later lessons, you will learn about linear classifiers. These algorithms work by defining a
  decision boundary that separates the different categories.

          decision boundary (graph picture)

    Looking back at the plots of the data, do you think a linear classifier would work well on
    these datasets?
    
```{r echo=TRUE}
# libraries to use
library(ggplot2)
# library(readxl)
# library(DataExplorer)
# library(ggm)
library(readr)
# library(dplyr)
# # library(stringr)
# # library(psych)
# # library(janitor)
# library(car)
# library(QuantPsyc)
# # library(pastecs)
# # library(sqldf)
library(caret)
library(class)

options(scipen=100)
options(digits=5)

# read the data files
binaryData <- read_csv("binary-classifier-data.csv")
trinaryData <- read_csv("trinary-classifier-data.csv")

```

## a. Plot the data from each dataset using a scatter plot.

```{r echo=FALSE}
ggplot(data = binaryData) +
  aes(x = x, y = y) +
  geom_point(color = '#0c4c8a') +
  labs(title = 'Binary Classifier Data',
    x = 'X data',
    y = 'Y data') +
  theme_grey()

ggplot(data = trinaryData) +
  aes(x = x, y = y) +
  geom_point(color = '#0c4c8a') +
  labs(title = 'Trinary Classifier Data',
    x = 'X data',
    y = 'Y data') +
  theme_grey()
```

## Fit a k nearest neighbors model for each dataset for k=3, k=5, k=10, k=15, k=20, and k=25. 
## Compute the accuracy of the resulting models for each value of k. Plot the results in a graph where the
## x-axis is the different values of k and the y-axis is the accuracy of the model.

### Create the required data structures
```{r echo=TRUE}
#set the seed
set.seed(1206)

#create the training and test sets
binTrain <- createDataPartition(y=binaryData$label, p=0.7, list = FALSE)
binTraining <- binaryData[binTrain,]
binTesting <- binaryData[-binTrain,]

triTrain <- createDataPartition(y=trinaryData$label, p=0.7, list = FALSE)
triTraining <- trinaryData[triTrain,]
triTesting <- trinaryData[-triTrain,]

#convert the label fields to factors
binLabel = factor(binTraining[["label"]])
triLabel = factor(triTraining[["label"]])

#check the dimensions of the data
dim(binTraining); dim(binTesting)
dim(triTraining); dim(triTesting)
```
## Do knn for the Binary Data
### Compute binary data for k=3
```{r echo=TRUE}
K <- 3
#create the knn model
binModelknn <- knn(binTraining, binTesting, binLabel, k=K)
binModelknn

#create the confusion matrix
tabBinTesting <- table(binModelknn,binTesting$label)
tabBinTesting

#compute the model's accuracy
accuracyBin3 <- (tabBinTesting[1,1] + tabBinTesting[2,2])/(tabBinTesting[1,1] + tabBinTesting[2,2]+tabBinTesting[1,2]+tabBinTesting[2,1])
accuracyBin3

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracy <- data.frame("K"=c(K), "Accuracy"=c(accuracyBin3))
str(frameAccuracy)
```
### Compute binary data for k=5
```{r echo=TRUE}
K <- 5
#create the knn model
binModelknn <- knn(binTraining, binTesting, binLabel, k=K)
binModelknn

#create the confusion matrix
tabBinTesting <- table(binModelknn,binTesting$label)
tabBinTesting

#compute the model's accuracy
accuracyBin5 <- (tabBinTesting[1,1] + tabBinTesting[2,2])/(tabBinTesting[1,1] + tabBinTesting[2,2]+tabBinTesting[1,2]+tabBinTesting[2,1])
accuracyBin5

#add values to the graphing data frame
frameAccuracy <- rbind(frameAccuracy,list(K,accuracyBin5))
str(frameAccuracy)
```
### Compute binary data for k=10
```{r echo=TRUE}
K <- 10
#create the knn model
binModelknn <- knn(binTraining, binTesting, binLabel, k=K)
binModelknn

#create the confusion matrix
tabBinTesting <- table(binModelknn,binTesting$label)
tabBinTesting

#compute the model's accuracy
accuracyBin10 <- (tabBinTesting[1,1] + tabBinTesting[2,2])/(tabBinTesting[1,1] + tabBinTesting[2,2]+tabBinTesting[1,2]+tabBinTesting[2,1])
accuracyBin10

#add values to the graphing data frame
frameAccuracy <- rbind(frameAccuracy,list(K,accuracyBin10))
str(frameAccuracy)
```
### Compute binary data for k=15
```{r echo=TRUE}
K <- 15
#create the knn model
binModelknn <- knn(binTraining, binTesting, binLabel, k=K)
binModelknn

#create the confusion matrix
tabBinTesting <- table(binModelknn,binTesting$label)
tabBinTesting

#compute the model's accuracy
accuracyBin15 <- (tabBinTesting[1,1] + tabBinTesting[2,2])/(tabBinTesting[1,1] + tabBinTesting[2,2]+tabBinTesting[1,2]+tabBinTesting[2,1])
accuracyBin15

#add values to the graphing data frame
frameAccuracy <- rbind(frameAccuracy,list(K,accuracyBin15))
str(frameAccuracy)
```
### Compute binary data for k=20
```{r echo=TRUE}
K <- 20
#create the knn model
binModelknn <- knn(binTraining, binTesting, binLabel, k=K)
binModelknn

#create the confusion matrix
tabBinTesting <- table(binModelknn,binTesting$label)
tabBinTesting

#compute the model's accuracy
accuracyBin20 <- (tabBinTesting[1,1] + tabBinTesting[2,2])/(tabBinTesting[1,1] + tabBinTesting[2,2]+tabBinTesting[1,2]+tabBinTesting[2,1])
accuracyBin20

#add values to the graphing data frame
frameAccuracy <- rbind(frameAccuracy,list(K,accuracyBin20))
str(frameAccuracy)
```
### Compute binary data for k=25
```{r echo=TRUE}
K <- 25
#create the knn model
binModelknn <- knn(binTraining, binTesting, binLabel, k=K)
binModelknn

#create the confusion matrix
tabBinTesting <- table(binModelknn,binTesting$label)
tabBinTesting

#compute the model's accuracy
accuracyBin25 <- (tabBinTesting[1,1] + tabBinTesting[2,2])/(tabBinTesting[1,1] + tabBinTesting[2,2]+tabBinTesting[1,2]+tabBinTesting[2,1])
accuracyBin25

#add values to the graphing data frame
frameAccuracy <- rbind(frameAccuracy,list(K,accuracyBin25))
str(frameAccuracy)
```
### Plot the binary data knn models results
```{r echo=FALSE}
ggplot(data = frameAccuracy) +
  aes(x = K, y = Accuracy) +
  geom_line(color = '#0c4c8a') +
  labs(title = 'Binary KNN Accuracy',
    x = 'K Value',
    y = 'Accuracy') +
  theme_grey()
```
## Do knn for the Trinary Data
### Compute trinary data for k=3
```{r echo=TRUE}
K <- 3
#create the knn model
triModelknn <- knn(triTraining, triTesting, triLabel, k=K)
triModelknn

#create the confusion matrix
tabTriTesting <- table(triModelknn,triTesting$label)
tabTriTesting

#compute the model's accuracy
accuracyTri3 <- (tabTriTesting[1,1] + tabTriTesting[2,2]+tabTriTesting[3,3])/(tabTriTesting[1,1] + tabTriTesting[1,2]+tabTriTesting[1,3]+
                                                                                tabTriTesting[2,1] + tabTriTesting[2,2]+tabTriTesting[2,3]+
                                                                                tabTriTesting[3,1] + tabTriTesting[3,2]+tabTriTesting[3,3])
                                                                                
accuracyTri3

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracyTri <- data.frame("K"=c(K), "Accuracy"=c(accuracyTri3))
str(frameAccuracyTri)
```
### Compute trinary data for k=5
```{r echo=TRUE}
K <- 5
#create the knn model
triModelknn <- knn(triTraining, triTesting, triLabel, k=K)
triModelknn

#create the confusion matrix
tabTriTesting <- table(triModelknn,triTesting$label)
tabTriTesting

#compute the model's accuracy
accuracyTri5 <- (tabTriTesting[1,1] + tabTriTesting[2,2]+tabTriTesting[3,3])/(tabTriTesting[1,1] + tabTriTesting[1,2]+tabTriTesting[1,3]+
                                                                                tabTriTesting[2,1] + tabTriTesting[2,2]+tabTriTesting[2,3]+
                                                                                tabTriTesting[3,1] + tabTriTesting[3,2]+tabTriTesting[3,3])

accuracyTri5

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracyTri <- rbind(frameAccuracyTri,list(K,accuracyTri5))
str(frameAccuracyTri)
```
### Compute trinary data for k=10
```{r echo=TRUE}
K <- 10
#create the knn model
triModelknn <- knn(triTraining, triTesting, triLabel, k=K)
triModelknn

#create the confusion matrix
tabTriTesting <- table(triModelknn,triTesting$label)
tabTriTesting

#compute the model's accuracy
accuracyTri10 <- (tabTriTesting[1,1] + tabTriTesting[2,2]+tabTriTesting[3,3])/(tabTriTesting[1,1] + tabTriTesting[1,2]+tabTriTesting[1,3]+
                                                                                tabTriTesting[2,1] + tabTriTesting[2,2]+tabTriTesting[2,3]+
                                                                                tabTriTesting[3,1] + tabTriTesting[3,2]+tabTriTesting[3,3])

accuracyTri10

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracyTri <- rbind(frameAccuracyTri,list(K,accuracyTri10))
str(frameAccuracyTri)
```
### Compute trinary data for k=15
```{r echo=TRUE}
K <- 15
#create the knn model
triModelknn <- knn(triTraining, triTesting, triLabel, k=K)
triModelknn

#create the confusion matrix
tabTriTesting <- table(triModelknn,triTesting$label)
tabTriTesting

#compute the model's accuracy
accuracyTri15 <- (tabTriTesting[1,1] + tabTriTesting[2,2]+tabTriTesting[3,3])/(tabTriTesting[1,1] + tabTriTesting[1,2]+tabTriTesting[1,3]+
                                                                                tabTriTesting[2,1] + tabTriTesting[2,2]+tabTriTesting[2,3]+
                                                                                tabTriTesting[3,1] + tabTriTesting[3,2]+tabTriTesting[3,3])

accuracyTri15

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracyTri <- rbind(frameAccuracyTri,list(K,accuracyTri15))
str(frameAccuracyTri)
```
### Compute trinary data for k=20
```{r echo=TRUE}
K <- 20
#create the knn model
triModelknn <- knn(triTraining, triTesting, triLabel, k=K)
triModelknn

#create the confusion matrix
tabTriTesting <- table(triModelknn,triTesting$label)
tabTriTesting

#compute the model's accuracy
accuracyTri20 <- (tabTriTesting[1,1] + tabTriTesting[2,2]+tabTriTesting[3,3])/(tabTriTesting[1,1] + tabTriTesting[1,2]+tabTriTesting[1,3]+
                                                                                tabTriTesting[2,1] + tabTriTesting[2,2]+tabTriTesting[2,3]+
                                                                                tabTriTesting[3,1] + tabTriTesting[3,2]+tabTriTesting[3,3])

accuracyTri20

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracyTri <- rbind(frameAccuracyTri,list(K,accuracyTri20))
str(frameAccuracyTri)
```
### Compute trinary data for k=25
```{r echo=TRUE}
K <- 25
#create the knn model
triModelknn <- knn(triTraining, triTesting, triLabel, k=K)
triModelknn

#create the confusion matrix
tabTriTesting <- table(triModelknn,triTesting$label)
tabTriTesting

#compute the model's accuracy
accuracyTri25 <- (tabTriTesting[1,1] + tabTriTesting[2,2]+tabTriTesting[3,3])/(tabTriTesting[1,1] + tabTriTesting[1,2]+tabTriTesting[1,3]+
                                                                                tabTriTesting[2,1] + tabTriTesting[2,2]+tabTriTesting[2,3]+
                                                                                tabTriTesting[3,1] + tabTriTesting[3,2]+tabTriTesting[3,3])

accuracyTri25

#create data frame for graphing k versus accuracy
#add values to the data frame
frameAccuracyTri <- rbind(frameAccuracyTri,list(K,accuracyTri25))
str(frameAccuracyTri)
```
### Plot the trinary data knn models results
```{r echo=FALSE}
ggplot(data = frameAccuracyTri) +
  aes(x = K, y = Accuracy) +
  geom_line(color = '#0c4c8a') +
  labs(title = 'Trinary KNN Accuracy',
       x = 'K Value',
       y = 'Accuracy') +
  theme_grey()
```


##  Looking back at the plots of the data, do you think a linear classifier would work well on these datasets?

A linear classifier would not work well for these data sets, because the x and y values don't appear to have a linear relationship, so the accuracy would probably not be sufficient to rely on the predictions that it computes.
---
title: "10.3 Assignment: Fit a Logistic Regression Model to Previous Dataset"
author: "Larry Heckel"
date: May 15, 2019
output: html_document

---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Assignment 10.3 Instructions

Include all of your answers in a R Markdown report. An example can be found here that you can use as a guide.

Fit a logistic regression model to the binary-classifier-data.csv dataset from the previous assignment.

a. What is the accuracy of the logistic regression classifier?

b. How does the accuracy of the logistic regression classifier compare to the nearest neighbors algorithm?

c. Why is the accuracy of the logistic regression classifier different from that of the nearest neighbors?



```{r echo=TRUE}
# libraries to use
library(ggplot2)
# library(readxl)
# library(DataExplorer)
# library(ggm)
library(readr)
library(dplyr)
# # library(stringr)
# # library(psych)
# # library(janitor)
# library(car)
# library(QuantPsyc)
# # library(pastecs)
# # library(sqldf)
#library(caret)
#library(class)
library(factoextra)
library(purrr)

options(scipen=100)
options(digits=5)

# read the data files
binClass <- read_csv("binary-classifier-data.csv")
glimpse(binClass)
set.seed(1206)
```
## Fit a logistic regression model to the binary-classifier-data.csv dataset from the previous assignment.
```{r echo=TRUE}
#create the model with the data set
binClassModel <- glm(label ~ ., data = binClass, family = "binomial")
summary(binClassModel)
```

## What is the accuracy of the logistic regression classifier?
```{r echo=TRUE}
#find the probabilities of every observation in the test data set
binClass$modelProb <- predict(binClassModel, binClass, type="response")

#Pick my decision threshold and set the 1's and 0's
binClass <- binClass %>% mutate(modelPredict = 1*(modelProb > .50) +0)
glimpse(binClass)

true_pos <- (binClass$label==1) & (binClass$modelPredict==1)
true_neg <- (binClass$label==0) & (binClass$modelPredict==0)
false_pos <- (binClass$label==0) & (binClass$modelPredict==1)
false_neg <- (binClass$label==1) & (binClass$modelPredict==0)

confMatrix <- matrix(c(sum(true_pos), sum(false_pos),
                       sum(false_neg), sum(true_neg)),
                     2,2)
colnames(confMatrix) <- c('Yhat=1', 'Yhat=0')
rownames(confMatrix) <- c('Y = 1', 'Y = 0')
confMatrix

regrAccuracy <- (sum(true_pos)+sum(true_neg)) / (sum(true_pos)+sum(true_neg)+sum(false_pos)+sum(false_neg))
regrAccuracy
```
I chose to go with a greater than 50% probability generated by the model for its predicting a "1" value. With this threshold, the model's accuracy is 58.344%. 

## How does the accuracy of the logistic regression classifier compare to the nearest neighbors algorithm?

Even at its worst accuracy, the knn classifier's accuracy is still over 97%, compared to the logistic regression's best accuracy of 58%. 

## Why is the accuracy of the logistic regression classifier different from that of the nearest neighbors?

With two variables, the logistic regression function attempts to create/fit a line that best classifies, or separates the data set into its two outcomes. Its function is to create a line, with a slope and y-intercept, to predict the outcome.

Given the x-y distribution of the data, as previously shown by scatter plot, attempting to fit a line to classify the outcomes is not an optimal solution. The scatter plot shows groupings of data, rather than a linear relationship. Is is straightforward to view the scatter plot and recognize that there is not a linear relationship  between the predictor and outcome variables.

The knn method, on the other hand, attempts to miminize the sum of squares of the Euclidean distance in order to predict the outcome. This method is not attempting to predict the outcome variable based on a line, but it assumes that the outcome variable exists in an n-dimensional space that can be predicted based on points that are close together. So when a new outcome is to be predicted, the model finds the reference point, k, that is can minimize the Euclidean distance to, and that becomes the predicted outcome. 

Given the distribution of this data set, then, the knn method is a better predictor than the logistic regression.

As a result, a knn solution, with a well-chosen k value, leads to a more accurate model in predicting the outcome.
